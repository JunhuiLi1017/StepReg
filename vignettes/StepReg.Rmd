---
title: "StepReg: Stepwise Regression Analysis"
author:
- name: Junhui Li
  affiliation: 
  - University of Massachusset Chan medical school, Worcester, USA
- name: Kai Hu
  affiliation: University of Massachusset Chan medical school, Worcester, USA
- name: Xiaohuan Lu
  affiliation: Clark University, Worcester, USA
- name: Julie Lihua Zhu
  affiliation: University of Massachusset Chan medical school, Worcester, USA
- name: Wenxin Liu
  affiliation: China Agricultural University, Beijing, China
package: StepReg
bibliography: bibliography.bib
fontsize: 11pt
nocite: '@*'
link-citations: true
output:
  BiocStyle::html_document:
    toc_float: true
  BiocStyle::pdf_document: default
abstract: |
  A tutorial on employing StepReg for stepwise regression analysis with four well-known datasets namely the mtcars, remission, lung, and CreditCard. The guild showcases the utility of StepReg across four well-known datasets, employing it for different regression models such as linear, logistic, Cox proportional hazard, and Poisson regression. The vignette elucidates the stepwise process with distinct parameters, offering users a clear understanding of how to effectively utilize StepReg for exploratory data analysis and model building in various regression scenarios.

vignette: |
  %\VignetteIndexEntry{StepReg: Stepwise Regression made it simple}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

Stepwise regression is a widely employed data-mining technique aimed at identifying a valuable subset of predictors for utilization in a multiple regression model. To facilitate this process, we have developed the R package StepReg. Depending on the nature of the response variable, StepReg facilitates users in conducting linear regression for continuous outcomes, logistic regression for binary outcomes, Cox regression for time-to-event outcomes, and Poisson regression for count outcomes, incorporating popular selection criteria. It provides a versatile set of stop rules available in forward selection, backward elimination, both-direction, and best subset methods.

Here, we applied the StepReg package to four well-established and diverse datasets---mtcars, remission, lung, and CreditCard---utilizing distinct parameters across various regression scenarios. These datasets provide robust test cases for showcasing the capabilities and versatility of the StepReg package in real-world applications. Through practical demonstrations, we illustrated the application of linear stepwise regression for continuous outcomes, logistic stepwise regression for binary outcomes, Cox stepwise regression for time-to-event outcomes, and Poisson stepwise regression for count outcomes. These examples offer users valuable insights into the effective utilization of StepReg for variable selection in different regression scenarios, providing a comprehensive guide for those seeking proficiency in incorporating StepReg into their analytical toolkit.

# Quick Demo

The following example selects an optimal linear regression model with the `mtcars` dataset.
```{r, message = FALSE}
library(StepReg)
devtools::load_all("~/GitHub/StepReg/")

data(mtcars)
formula <- mpg ~ .
res <- stepwise(formula = formula,
                data = mtcars,
                type = "linear",
                include = c("qsec"),
                strategy = "bidirection",
                metric = c("AIC"))
```
Breakdown of the parameters:

+ `formula`: specifies the dependent and independent variables
+ `type`: specifies the regression category, depending on your data, choose from "linear", "logit", "cox", etc.
+ `include`: specifies the variables that must be in the final model
+ `strategy`: specifies the model selection strategy, choose from "forward", "backward", "bidirection", "subset"
+ `metric`: specifies the model fit evaluation information criterion, choose one or more from "AIC", "AICc", "BIC", etc.

The output consists of multiple tables, which can be viewed with:
```{r, message = FALSE}
res
```

You can also visualize the variable selection procedures with:
```{r, message = FALSE}
plot(res)
```
The `(+)1` refers to original model with intercept being added, `(+)` indicates variables being added to the model while `(-)` means variables being removed from the model.

Additionally, you can generate reports of various formats with:
```{r, eval = FALSE}
report(res, report_name = "path_to/demo_res", format = "html")
```
Replace `"path_to/demo_res"` with desired output file name, the suffix `".html"` will added automatically. Supported `format` includeds "html", "pdf", "docx", etc. For detailed examples and more usage, refer to section xxx () and xxx ().

# Key features

## Regression categories

**StepReg** supports multiple regressions, including *linear*, *logit*, *cox*, *poisson*, and *gamma* regressions. These methods primarily vary by the type of response variable (refer to the table below). Additional regression techniques can be incorporated upon user request.

```{r, echo = FALSE}
library(knitr)
library(kableExtra)

Regression <- c("linear", "logit", "cox", "poisson", "gamma")
Reponse <- c("continuous", "binary", "time-to-event", "count", "time-series")
df <- data.frame(Regression, Reponse)

kable(df, format = "html", caption = 'Common regression categories') %>% kable_styling()
```

## Model selection strategies

Model selection aims to identify the subset of independent variables that provide the best predictive performance for the response variable. Both stepwise regression and best subsets approaches are implemented in StepReg. For stepwise regression, there are mainly three methods: *Forward Selection*, *Backward Elimination*, *Bidirectional Elimination*.

```{r, echo = FALSE}
Strategy <- c("Forward Selection", "Backward Elimination", "Bidirectional Elimination", "Best Subsets")
Description <- c("In forward selection, the algorithm starts with an empty model (no predictors) and adds in variables one by one. Each step tests the addition of every possible predictor by calculating a pre-selected metric. Add the variable (if any) whose inclusion leads to the most statistically significant fit improvement. Repeat this process until more predictors no longer lead to a statistically better fit.",
                "In backward elimination, the algorithm starts with a full model (all predictors) and deletes variables one by one. Each step test the deletion of every possible predictor by calculating a pre-selected metric. Delete the variable (if any) whose loss leads to the most statistically significant fit improvement. Repeat this process until less predictors no longer lead to a statistically better fit.",
                "Bidirectional elimination is essentially a forward selection procedure combined with backward elimination at each iteration. Each iteration starts with a forward selection step that adds in predictors, followed by a round of backward elimination that removes predictors. Repeat this process until no more predictors are added or excluded.",
                "Stepwise algorithms add or delete one predictor at a time and output a single model without evaluating all candidates. Therefore, it is a relatively simple procedure that only produces one model. In contrast, the *Best Subsets* algorithm calculates all possible models and output the best-fitting models with one predictor, two predictors, etc., for users to choose from.")
df <- data.frame(Strategy, Description)

kable(df, format = "html", caption = 'Model selection strategy') %>% kable_styling()
```

Given the computational constraints, when dealing with datasets featuring a substantial number of predictor variables greater than the sample size, the Bidirectional Elimination typically emerges as the most advisable approach. Forward Selection and Backward Elimination can be considered in sequence. On the contrary, the Best Subsets approach requires the most substantial processing time, yet it calculates a comprehensive set of models with varying numbers of variables. In practice, users can experiment with various methods and select a final model based on the specific dataset and research objectives at hand.

## Selection criteria

*Selection criteria*, also referred to as *selection metrics* or *metrics*, is a means of evaluating a model's performance, which typically constitutes two parts: the accuracy of the model fitting the data and the complexity of the model. A selection metric must typically balance the two parts. In practice, multiple metrics have been proposed, the ones supported by StepReg are summarized below. Importantly, given the discrepancies in terms of the precise definitions of each metric, StepReg mirrors the formulas adopted by [SAS](https://documentation.sas.com/doc/en/statcdc/14.2/statug/statug_glmselect_details15.htm) for *univariate multiple regression (UMR)* except for HQ, IC(1), and IC(3/2). A subset of the UMR can be easily extended to *multivariate multiple regression (MMR)*, which are indicated in the following table.

```{r, echo = FALSE}
Statistic <- c(
"${n}$",
"${p}$",
"${q}$",
"$\\sigma^2$",
"${SST}$",
"${SSE}$",
"$\\text{LL}$",
"${|  |}$",
"$\\ln()$")

Meanings <- c(
"Sample Size",
"Number of parameters including the intercept",
"Number of dependent variables",
"Estimate of pure error variance from fitting the full model, which is the average of sigma value when it is for multivariate multiple regression",
"Total sum of squares corrected for the mean for the dependent variable, which is a numeric value for UMR and a matrix for multivariate regression",
"Error sum of squares, which is a numeric value for UMR and a matrix for multivariate regression",
"The natural logarithm of likelihood",
"The determinant function",
"The natural logarithm")

kable_styling(kable(data.frame(Statistic,Meanings),format = "html", align='l', escape = F, caption = 'Statistics in Information Criterion'))
```

```{r, echo = FALSE}
Abbreviation <- c("", "AIC", "AICc", "BIC", "CP", "HQ", "IC(1)", "IC(3/2)", "SBC", "SL", "Rsq", "adjRsq")
Definition <- c("",
                "Akaike’s Information Criterion",
                "Corrected Akaike’s Information Criterion",
                "Sawa Bayesian Information Criterion",
                "Mallows’ Cp statistic",
                "Hannan and Quinn Information Criterion",
                "Information Criterion with Penalty Coefficient Set to 1",
                "Information Criterion with Penalty Coefficient Set to 3/2",
                "Schwarz Bayesian Information Criterion",
                "Significance Level (pvalue)",
                "R-square statistic",
                "Adjusted R-square statistic")

Formula_in_Linear <- c("linear",
                       "$n\\ln\\left(\\frac{|\\text{SSE}|}{n}\\right) + 2pq + n + q(q+1)$ <br>[@Hurvich_Tsai_1989; @Al-Subaihi_2002]$^1$",
                       "$n\\ln\\left(\\frac{|\\text{SSE}|}{n}\\right) + \\frac{nq(n+p)}{n-p-q-1}$ <br>[@Hurvich_Tsai_1989; @Bedrick_Tsai_1994]$^2$",
                       "$n\\ln\\left(\\frac{SSE}{n}\\right) + 2(p+2)o - 2o^2, o = \\frac{n\\sigma^2}{SSE}$ <br>[@Sawa_1978; @Judge_1985] <br>not available for MMR",
                       "$\\frac{SSE}{\\sigma^2} + 2p - n$ <br> [@Mallows_1973; @Hocking_1976] <br>not available for MMR",
                       "$n\\ln\\left(\\frac{|\\text{SSE}|}{n}\\right) + 2pq\\ln(\\ln(n))$ <br>[@Hannan_Quinn_1979; @McQuarrie_Tsai_1998; @Hurvich_Tsai_1989]",
                       "$n\\ln\\left(\\frac{|\\text{SSE}|}{n}\\right) + p$ <br>[@Nelder_Wedderburn_1972; @Smith_Spiegelhalter_1980] not available for MMR",
                       "$n\\ln\\left(\\frac{|\\text{SSE}|}{n}\\right) + \\frac{3}{2}p$ <br>[@Smith_Spiegelhalter_1980] <br>not available for MMR",
                       "$n\\ln\\left(\\frac{|\\text{SSE}|}{n}\\right) + p \\ln(n)$ <br>[@Hurvich_Tsai_1989; @Schwarz_1978; @Judge_1985; @Al-Subaihi_2002] <br>not available for MMR",
                       "$\\textit{F test}$ for UMR and $\\textit{Approximate F test}$ for MMR",
                       "$1 - \\frac{SSE}{SST}$ <br>not available for MMR",
                       "$1 - \\frac{(n-1)(1-R^2)}{n-p}$ <br>[@Darlington_1968; @Judge_1985] <br>not available for MMR")

Formula_in_Logit_Cox_Poisson_Gamma <- c("logit, cox, poisson and gamma",
                                        "$-2\\text{LL} + 2p$ <br>[@Darlington_1968; @Judge_1985]",
                                        "$-2\\text{LL} + \\frac{n(n+p)}{n-p-2}$ <br>[@Hurvich_Tsai_1989]",
                                        "not available",
                                        "not available",
                                        "$-2\\text{LL} + 2p\\ln(\\ln(n))$ <br>[@Hannan_Quinn_1979]",
                                        "$-2\\text{LL} + p$ <br>[@Nelder_Wedderburn_1972; @Smith_Spiegelhalter_1980]",
                                        "$-2\\text{LL} + \\frac{3}{2}p$ <br>[@Smith_Spiegelhalter_1980]",
                                        "$-2\\text{LL} + p\\ln(n)$ <br>[@Schwarz_1978; @Judge_1985]",
                                        "Forward: LRT and Rao Chi-square test (logit, poisson, gamma); LRT (cox); <br><br>Backward: Wald test",
                                        "not available",
                                        "not available")
df <- data.frame(Abbreviation, Definition, Formula_in_Linear, Formula_in_Logit_Cox_Poisson_Gamma)
colnames(df) <- c("Abbreviation","Definition","Formula","")

kable(df, format = "html", align = "l", 
      booktabs = TRUE, escape = F, 
      caption = 'Abbreviation, Definition, and Formula of the Information Criterion for Linear, Logit, Cox, Possion, and Gamma regression') %>%
  footnote(number = c("Unsupported AIC formula (which does not affect the selection process as it only differs by constant additive and multiplicative factors):\n
                      $AIC=n\\ln\\left(\\frac{SSE}{n}\\right) + 2p$ [@Darlington_1968; @Judge_1985]", 
                      "Unsupported AICc formula (which does not affect the selection process as it only differs by constant additive and multiplicative factors):\n
                      $AICc=\\ln\\left(\\frac{SSE}{n}\\right) + 1 + \\frac{2(p+1)}{n-p-2}$ [@McQuarrie_Tsai_1998]")) %>%
  kable_styling() %>%
  column_spec(3, width = "0.5in") %>%
  column_spec(4, width = "0.4in")
```

In practice, no information criterion is necessarily optimal for prediction for all data sets. The choice of them depends on your specific data and research goals. Below summarizes general guidance.

+ AIC: AIC works by penalizing the inclusion of additional variables in a model. The lower the AIC, the better performance of the model. AIC does not include sample size in penalty calculation, and it is optimal in minimizing the mean square error of predictions [1]().
+ AICc: AICc is a variant of AIC, which works better for small sample size, especially when `numObs / numParam < 40` [2]().
+ CP: Another variant of AIC, it is equivalent to AIC in terms of model selection.
+ IC(1) and IC(3/2): IC(1) and IC(3/2) have 1 and 3/2 as penalty factors respectively, compared to 2 used by AIC. As such, IC(1) turns to return a complex model with more variables that may suffer from overfitting issues.
+ BIC and SBC: Both BIC and SBC are variants of Bayesian Information Criterion. The precise definition of them differ in literature and in SAS. Here, BIC mirrors the definition in SAS (Sawa Bayesion Information Criterion) while SBC refers to Schwarz Bayesian Information Criterion. Compared to AIC, the only difference is that Bayesian Information Criterion penalizes the sample size in formula. In practice, SBC is more likely to return fewer variables that may cause the model to under fit [3, (Sawa 1978; Judge et al. 1985; Swacht, 1978)].
+ HQ: An alternative to AIC, but have seen little use in practice. [4] 
+ SL: 
+ Rsq
+ adjRsq
+

1. https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/2041-210x.12541
2. https://www.amazon.com/Model-Selection-Multimodel-Inference-Information-Theoretic/dp/0387953647
3. SAS user guide
4. 


in the sense of minimizing the mean square error of predictions,
+ AICc:
+ BIC:

```{r}
# table goes here


Abbreviation <- c("AIC", "AICc", "BIC")

Use_case <- c("")


AIC: 
AICc: better for small sample size, especially when numObs / numParam < 40 []()
BIC: penalize more on number of parameters

```

AIC:
AICc: The analysis in [3] suggests using AICc when numObs/numParam < 40.
SBC:

Information criterion:
Relationship to R² and the Adjusted-R²
Relation to p-values and statistical significance

In practice, considering the unique characteristics of the data and the objectives of the analysis within each field, we advise users to employ different selection criteria for conducting stepwise regression parallely, which allows users to choose the optimal model according to their specific requirements.

## Multicollinearity

This [blog](https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/) by Jim Frost gives an excellent overview of multicollinearity and when it is necessary to remove it.

Simply put, a dataset contains multicollinearity when input predictors are correlated. When multicollinearity occurs, the interpretability of predictors will be badly affected because changes in one input variable lead to changes in other input variables. Therefore, it is hard to individually estimate the relationship between each input variable and the dependent variable.

Multicollinearity can dramatically reduce the precision of the estimated regression coefficients of correlated input variables, making it hard to find the correct model. However, as Jim pointed out, “Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity.”

In StepReg, [QC Matrix Decomposition](https://towardsdatascience.com/qr-matrix-factorization-15bae43a6b2#:~:text=The%20QR%20matrix%20decomposition%20allows%20one%20to%20express%20a%20matrix,zero%2C%20it%20is%20also%20invertible.) is performed ahead of time to detect and remove input variables causing multicollinearity.

# StepReg Output

A list containing multiple tables will be returned. 

```{r, echo = FALSE}
Table <- c(
"Summary of Parameters",
"Variables and Types",
"Selection Process under IC",
"Parameter Estimates for y under IC")

Meanings <- c(
"The parameters utilized in stepwise regression along with their default or user-specified values",
"The variables and their respective types in the dataset",
"The detailed overview of the variable selection process under information criteria IC.",
"The parameter estimates for the optimal models under information criteria IC")

kable_styling(kable(data.frame(Table,Meanings),format = "html", align='l', escape = F, caption = 'Output Explanation of StepReg')) #%>% column_spec(1:2, width = c("2in","4in")))
```

# Demo

In this section, we provided some examples using distinct parameters across various regression scenarios with the 4 datasets, a.k.a. mtcars, remission, lung, and CreditCard.

## linear stepwise regression with _mtcars_

-   [**mtcars**](https://cran.r-project.org/web/packages/explore/vignettes/explore_mtcars.html): the mtcars dataset is a classic automotive dataset that provides information on various car models and their performance attributes. With 32 observations and 11 variables, it includes details such as miles per gallon (mpg), horsepower(hp), and the number of cylinders(cyl). For more information, please ```?mtcars```

> **Example1**:  
>
> type of regression: `linear`
>
> response: `mpg`
>
> predictors: all variables except `mpg` 
>
> variable selection strategy: `forward` 
>
> information criteria: `AIC` 
>
> force `disp` and `cyl` to be included in all models.

```{r}
    library(StepReg)
    data(mtcars)
    formula <- mpg ~ .
    exam1 <- stepwise(formula = formula,
                      data = mtcars,
                      type = "linear",
                      include = c("disp","cyl"),
                      strategy = "forward",
                      metric = "AIC")
    exam1
```

> Visulization of the selection process under `AIC`.

```{r plot_exam1, warning=FALSE}
    plot(exam1)
```

> **Example2**: 
>
> type of regression: `linear`
>
> response: `mpg` 
>
> predictors: all other variables except `mpg` and `intercept`. 
>
> variable selection strategy: `bidirectional` 
>
> information criterion: run `AIC`, `AICc`, `BIC`,`HQ`, `HQc`, `SBC`, and `SL` parallelly, and the significance levels for entry (`sle`) and stay (`sls`) were both set to 0.05 . 
>
> Users can compare output within each metic through the output table and visualization.

```{r}
    formula <- mpg ~ . + 0
    exam2 <- stepwise(formula = formula,
                      data = mtcars,
                      type = "linear",
                      strategy = "bidirection",
                      metric = c("AIC","SBC","SL","AICc","BIC","HQ"),
                      sle = 0.05,
                      sls = 0.05)
    exam2
```

> Visulization of the selection process using `bidirection` strategy under information criteron `AIC`, `AICc`, `BIC`,`HQ`, `SBC`, and `SL` with `sle`=0.05 and `sls`=0.05. 

```{r plot_exam2, warning=FALSE}
    plot(exam2)
```

> **Example3**: 
>
> type of regression: `linear`
>
> response: multivariates of `mpg` and `drat` 
>
> predictors: `cyl`, `disp`, `hp`, `wt`, `vs`, `am` and `intercept`. 
>
> variable selection strategy: `subset` 
>
> information criterion: run `AIC`, `SBC` and `HQ` parallelly

```{r}
    formula <- cbind(mpg,drat) ~ cyl + disp + hp + wt + vs + am
    exam3 <- stepwise(formula = formula,
                      data = mtcars,
                      type = "linear",
                      include = 'wt',
                      strategy = "subset",
                      metric = c("AIC","AICc","SBC","HQ"),
                      best_n = 3)
    exam3
```

> Visulization of the selection process using `subset` strategy under information criteron `AIC`, `SBC` and `HQ`.

```{r plot_exam3, warning=FALSE}
    plot(exam3)
```

## Logistic stepwise regression with `remission`

-   [**remission**](https://online.stat.psu.edu/stat501/book/export/html/1011): the remission dataset is relevant in the context of medical research, specifically in oncology. It captures data related to the remission status of leukemia patients. With 27 observations and 7 variables, the dataset includes variables such as remission status (1 for remission and 0 for non-remission), cellularity of the marrow clot section(cell), and the highest temperature before the start of treatment(temp). For more information, please ```?StepReg::remission```


> **Example4**: 
>
> type of regression: `logit`
>
> response: `remiss` 
>
> predictors: all variables except `remiss`
>
> variable selection strategy: `forward` 
>
> information criterion: run `AIC` and `SL` parallelly, where `sle` and `sls` were both set to 0.05.
>
> force `cell` always in the model.

```{r warning = FALSE}
    data(remission)
    formula <- remiss ~ .
    exma4 <- stepwise(formula = formula,
                      data = remission,
                      type = "logit",
                      include= "cell",
                      strategy = "forward",
                      metric = c("AIC","SL"),
                      sle = 0.05)
    exma4
```

> Visulization of the selection process using `forward` strategy under information criteron `AIC` and `SL`.

```{r plot_exam4, warning=FALSE}
    plot(exma4)
```

> **Example5**: 
>
> type of regression: `logit`
>
> response: `remiss` 
>
> predictors: all variables except `remiss`
>
> variable selection strategy: `subset` 
>
> information criterion: run `SBC` and `SL` parallelly, where `sle` and `sls` used default 0.15.

```{r warning = FALSE}
    data(remission)
    formula <- remiss ~ .
    exma5 <- stepwise(formula = formula,
                      data = remission,
                      type = "logit",
                      strategy = "subset",
                      metric = c("SBC","SL"),
                      best_n = 3)
    exma5
```

> Visulization of the selection process using `subset` strategy under information criteron `SBC` and `SL`.

```{r plot_exam5, warning=FALSE}
    plot(exma5)
```

## Cox stepwise regression with `lung`

-   [**lung**](https://stat.ethz.ch/R-manual/R-devel/library/survival/html/lung.html): the lung dataset is a dataset in the survival analysis domain, containing information related to the survival times of 228 patients with advanced lung cancer. It includes variables such as the patient's age, the type of treatment received, and survival status. For more information, please ```?survival::lung```

> **Example6**: 
>
> type of regression: `cox`
>
> response: `Surv(time, status_binary)`
>
> predictors: all variables except `status`
>
> variable selection strategy: `forward` 
>
> information criterion: run `IC(1)` and `SL` parallelly, where `sle` was set to 0.05.
>
> force `age` in all models.

```{r warning = FALSE}
    lung <- survival::lung
    lung_noNA <- na.omit(lung)
    lung_noNA$status_binary <- ifelse(lung_noNA$status == 2,1,0)
    formula  =  Surv(time, status_binary) ~ . - status
    
    exma6 <- stepwise(formula = formula,
                      data = lung_noNA,
                      type = "cox",
                      include = "age",
                      strategy = "forward",
                      metric = c("IC(1)","SL"),
                      sle = 0.05)
    exma6
```

> Visulization of the selection process using `forward` strategy under information criteron `IC(1)` and `SL`.

```{r plot_exam6, warning=FALSE}
    plot(exma6)
```

> **Example7**: 
>
> type of regression: `cox`
>
> response: `Surv(time, status_binary)`
>
> predictors: all variables except `status`
>
> variable selection strategy: `backward` 
>
> information criterion: run `SL` and `AIC` parallelly, where `sls` was set to 0.05.

```{r warning = FALSE}
    formula = Surv(time, status_binary) ~ . - status 
    exma7 <- stepwise(formula = formula,
                      data = lung_noNA,
                      type = "cox",
                      strategy = "backward",
                      metric = c("SL","AIC"),
                      sls = 0.05)
    exma7
```

> Visulization of the selection process using `backward` strategy under information criteron `AIC` and `SL`.

```{r plot_exam7, warning=FALSE}
    plot(exma7)
```

## Poisson stepwise regression with `CreditCard`

-   [**CreditCard**](https://search.r-project.org/CRAN/refmans/AER/html/CreditCard.html): the CreditCard dataset is associated with credit risk analysis and financial research. It contains information about credit card transactions, including details such as the amount spent, credit limit, and payment status. For more information, please ```?AER::CreditCard```

> **Example8**: 
>
> type of regression: `poisson`
>
> response: `reports`
>
> predictors: all variables except `reports`
>
> variable selection strategy: `forward` 
>
> information criterion: run `SL` and `IC(3/2)` parallelly, and `sle` was set to 0.05.

```{r warning = FALSE}
    data(CreditCard, package = 'AER')
    formula  = reports ~ .
    
    exma8 <- stepwise(formula = formula,
                      data = CreditCard,
                      type = "poisson",
                      strategy = "forward",
                      metric = c("SL","IC(3/2)"),
                      sle=0.05)
    exma8
```

> Visulization of the selection process using `forward` strategy under information criteron `IC(3/2)` and `SL`.

```{r plot_exam8, warning=FALSE}
    plot(exma8)
```

> **Example9**: 
>
> type of regression: `poisson`
>
> response: `reports`
>
> predictors: all variables except `reports`
>
> variable selection strategy: `bidirection` 
>
> information criterion: run `AIC` and `IC(3/2)` parallelly
>
> force `card` and `months` in all models.

```{r warning = FALSE}
    formula = reports ~ .
    exma9 <- stepwise(formula = formula,
                      data = CreditCard,
                      type = "poisson",
                      include=c("card","months"),
                      strategy = "bidirection",
                      metric = c("IC(3/2)","AIC")
                      )
    exma9
```

> Visulization of the selection process using `bidirection` strategy under information criteron `IC(3/2)` and `AIC`.

```{r plot_exam9, warning=FALSE}
    plot(exma9)
```

# Session Info

```{r sessionInfo, echo = FALSE}
sessionInfo()
```