---
title: "StepReg: Stepwise Regression Analysis"
author:
- name: Junhui Li
  affiliation: 
  - University of Massachusset Chan medical school, Worcester, USA
- name: Kai Hu
  affiliation: University of Massachusset Chan medical school, Worcester, USA
- name: Xiaohuan Lu
  affiliation: Clark University, Worcester, USA
- name: Wenxin Liu
  affiliation: China Agricultural University, Beijing, China
- name: Julie Lihua Zhu
  affiliation: University of Massachusset Chan medical school, Worcester, USA
package: StepReg
bibliography: bibliography.bib
fontsize: 11pt
nocite: '@*'
link-citations: true
output:
  BiocStyle::html_document:
    toc_float: true
  BiocStyle::pdf_document: default
abstract: |
  A guild on using StepReg for stepwise regression analysis with four well-known datasets namely the mtcars, remission, lung, and CreditCard for different regression models such as linear, logistic, Cox proportional hazard, and Poisson regression. The vignette explains the stepwise process with distinct parameters, offering users a clear understanding of how to effectively utilize StepReg for exploratory data analysis and model building in various regression scenarios.
  
  StepReg is a comprehensive package aiming to simplify the stepwise regression procedures. 
  
  
  for simplifies the model selection procedure 
  
  
  A guild on using StepReg for stepwise regression analysis with four well-known datasets namely the mtcars, remission, lung, and CreditCard for different regression models such as linear, logistic, Cox proportional hazard, and Poisson regression. The vignette explains the stepwise process with distinct parameters, offering users a clear understanding of how to effectively utilize StepReg for exploratory data analysis and model building in various regression scenarios.

vignette: |
  %\VignetteIndexEntry{StepReg: Stepwise Regression made it simple}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Introduction

Stepwise regression is a widely employed data-mining technique aimed at identifying a valuable subset of predictors for utilization in a multiple regression model. To facilitate this process, we have developed the R package StepReg. Depending on the nature of the response variable, StepReg facilitates users in conducting linear regression for continuous outcomes, logistic regression for binary outcomes, Cox regression for time-to-event outcomes, and Poisson regression for count outcomes, incorporating popular selection criteria. It provides a versatile set of stop rules available in forward selection, backward elimination, both-direction, and best subset methods.

Here, we applied the StepReg package to four well-established and diverse datasets---mtcars, remission, lung, and CreditCard---utilizing distinct parameters across various regression scenarios. These datasets provide robust test cases for showcasing the capabilities and versatility of the StepReg package in real-world applications. Through practical demonstrations, we illustrated the application of linear stepwise regression for continuous outcomes, logistic stepwise regression for binary outcomes, Cox stepwise regression for time-to-event outcomes, and Poisson stepwise regression for count outcomes. These examples offer users valuable insights into the effective utilization of StepReg for variable selection in different regression scenarios, providing a comprehensive guide for those seeking proficiency in incorporating StepReg into their analytical toolkit.

# Quick Demo

The following example selects an optimal linear regression model with the `mtcars` dataset.
```{r, message = FALSE}
library(StepReg)

data(mtcars)
formula <- mpg ~ .
res <- stepwise(formula = formula,
                data = mtcars,
                type = "linear",
                include = c("qsec"),
                strategy = "bidirection",
                metric = c("AIC"))
```
Breakdown of the parameters:

+ `formula`: specifies the dependent and independent variables
+ `type`: specifies the regression category, depending on your data, choose from "linear", "logit", "cox", etc.
+ `include`: specifies the variables that must be in the final model
+ `strategy`: specifies the model selection strategy, choose from "forward", "backward", "bidirection", "subset"
+ `metric`: specifies the model fit evaluation metric, choose one or more from "AIC", "AICc", "BIC", "SL", etc.

The output consists of multiple tables, which can be viewed with:
```{r, message = FALSE}
res
```

You can also visualize the variable selection procedures with:
```{r, message = FALSE}
plot(res)
```
The `(+)1` refers to original model with intercept being added, `(+)` indicates variables being added to the model while `(-)` means variables being removed from the model.

Additionally, you can generate reports of various formats with:
```{r, eval = FALSE}
report(res, report_name = "path_to/demo_res", format = "html")
```
Replace `"path_to/demo_res"` with desired output file name, the suffix `".html"` will added automatically. Supported `format` includeds "html", "pdf", "docx", etc. For detailed examples and more usage, refer to section xxx () and xxx ().

# Key features

## Regression categories

**StepReg** supports multiple types of regressions, including *linear*, *logit*, *cox*, *poisson*, and *gamma* regressions. These methods primarily vary by the type of response variable (refer to the table below). Additional regression techniques can be incorporated upon user request.

```{r, echo = FALSE}
library(knitr)
library(kableExtra)

Regression <- c("linear", "logit", "cox", "poisson", "gamma")
Reponse <- c("continuous", "binary", "time-to-event", "count", "gamma distribution")
df <- data.frame(Regression, Reponse)

kable(df, format = "html", caption = 'Common regression categories') %>% kable_styling()
```

## Model selection strategies

Model selection aims to identify the subset of independent variables that provide the best predictive performance for the response variable. Both stepwise regression and best subsets approaches are implemented in StepReg. For stepwise regression, there are mainly three methods: *Forward Selection*, *Backward Elimination*, *Bidirectional Elimination*.

```{r, echo = FALSE}
Strategy <- c("Forward Selection", "Backward Elimination", "Bidirectional Elimination", "Best Subsets")
Description <- c("In forward selection, the algorithm starts with an empty model (no predictors) and adds in variables one by one. Each step tests the addition of every possible predictor by calculating a pre-selected metric. Add the variable (if any) whose inclusion leads to the most statistically significant fit improvement. Repeat this process until more predictors no longer lead to a statistically better fit.",
                "In backward elimination, the algorithm starts with a full model (all predictors) and deletes variables one by one. Each step test the deletion of every possible predictor by calculating a pre-selected metric. Delete the variable (if any) whose loss leads to the most statistically significant fit improvement. Repeat this process until less predictors no longer lead to a statistically better fit.",
                "Bidirectional elimination is essentially a forward selection procedure combined with backward elimination at each iteration. Each iteration starts with a forward selection step that adds in predictors, followed by a round of backward elimination that removes predictors. Repeat this process until no more predictors are added or excluded.",
                "Stepwise algorithms add or delete one predictor at a time and output a single model without evaluating all candidates. Therefore, it is a relatively simple procedure that only produces one model. In contrast, the *Best Subsets* algorithm calculates all possible models and output the best-fitting models with one predictor, two predictors, etc., for users to choose from.")
df <- data.frame(Strategy, Description)

kable(df, format = "html", caption = 'Model selection strategy') %>% kable_styling()
```

Given the computational constraints, when dealing with datasets featuring a substantial number of predictor variables greater than the sample size, the Bidirectional Elimination typically emerges as the most advisable approach. Forward Selection and Backward Elimination can be considered in sequence. On the contrary, the Best Subsets approach requires the most substantial processing time, yet it calculates a comprehensive set of models with varying numbers of variables. In practice, users can experiment with various methods and select a final model based on the specific dataset and research objectives at hand.

## Selection metrics

Various *selection metrics* can be used to guide the process of adding or removing predictors from the model. These metrics help to determine the importance or significance of predictors in improving the model fit. In StepReg, selection metrics include two categories: *Information Criteria* and *Significance Level* of the coefficient associated with each predictor. *Information Criteria* is a means of evaluating a model's performance, which balances model fit with complexity by penalizing models with a higher number of parameters. Lower *Information Criteria* values indicate a better trade-off between model fit and complexity. Note that when evaluating different models, it's important to compare them within the same *Information Criteria* framework rather than across multiple Information Criteria. For example, if you decide to use AIC, you should compare all models using AIC. This ensures consistency and fairness in model comparison, as each Information Criterion has its own scale and penalization factors. In practice, multiple metrics have been proposed, the ones supported by StepReg are summarized below.

Importantly, given the discrepancies in terms of the precise definitions of each metric, StepReg mirrors the formulas adopted by [SAS](https://documentation.sas.com/doc/en/statcdc/14.2/statug/statug_glmselect_details15.htm) for *univariate multiple regression (UMR)* except for HQ, IC(1), and IC(3/2). A subset of the UMR can be easily extended to *multivariate multiple regression (MMR)*, which are indicated in the following table.

```{r, echo = FALSE}
Statistic <- c(
"${n}$",
"${p}$",
"${q}$",
"$\\sigma^2$",
"${SST}$",
"${SSE}$",
"$\\text{LL}$",
"${|  |}$",
"$\\ln()$")

Meanings <- c(
"Sample Size",
"Number of parameters including the intercept",
"Number of dependent variables",
"Estimate of pure error variance from fitting the full model",
"Total sum of squares corrected for the mean for the dependent variable, which is a numeric value for UMR and a matrix for multivariate regression",
"Error sum of squares, which is a numeric value for UMR and a matrix for multivariate regression",
"The natural logarithm of likelihood",
"The determinant function",
"The natural logarithm")

kable_styling(kable(data.frame(Statistic,Meanings),format = "html", align='l', escape = F, caption = 'Statistics in selection metric'))
```

```{r, echo = FALSE}
Abbreviation <- c("", "AIC", "AICc", "BIC", "Cp", "HQ", "IC(1)", "IC(3/2)", "SBC", "SL", "Rsq", "adjRsq")
Definition <- c("",
                "Akaike’s Information Criterion",
                "Corrected Akaike’s Information Criterion",
                "Sawa Bayesian Information Criterion",
                "Mallows’ Cp statistic",
                "Hannan and Quinn Information Criterion",
                "Information Criterion with Penalty Coefficient Set to 1",
                "Information Criterion with Penalty Coefficient Set to 3/2",
                "Schwarz Bayesian Information Criterion",
                "Significance Level (pvalue)",
                "R-square statistic",
                "Adjusted R-square statistic")

Formula_in_Linear <- c("linear",
                       "$n\\ln\\left(\\frac{|\\text{SSE}|}{n}\\right) + 2pq + n + q(q+1)$ <br>[@Hurvich_Tsai_1989; @Al-Subaihi_2002]$^1$",
                       "$n\\ln\\left(\\frac{|\\text{SSE}|}{n}\\right) + \\frac{nq(n+p)}{n-p-q-1}$ <br>[@Hurvich_Tsai_1989; @Bedrick_Tsai_1994]$^2$",
                       "$n\\ln\\left(\\frac{SSE}{n}\\right) + 2(p+2)o - 2o^2, o = \\frac{n\\sigma^2}{SSE}$ <br>[@Sawa_1978; @Judge_1985] <br>not available for MMR",
                       "$\\frac{SSE}{\\sigma^2} + 2p - n$ <br> [@Mallows_1973; @Hocking_1976] <br>not available for MMR",
                       "$n\\ln\\left(\\frac{|\\text{SSE}|}{n}\\right) + 2pq\\ln(\\ln(n))$ <br>[@Hannan_Quinn_1979; @McQuarrie_Tsai_1998; @Hurvich_Tsai_1989]",
                       "$n\\ln\\left(\\frac{|\\text{SSE}|}{n}\\right) + p$ <br>[@Nelder_Wedderburn_1972; @Smith_Spiegelhalter_1980] not available for MMR",
                       "$n\\ln\\left(\\frac{|\\text{SSE}|}{n}\\right) + \\frac{3}{2}p$ <br>[@Smith_Spiegelhalter_1980] <br>not available for MMR",
                       "$n\\ln\\left(\\frac{|\\text{SSE}|}{n}\\right) + p \\ln(n)$ <br>[@Hurvich_Tsai_1989; @Schwarz_1978; @Judge_1985; @Al-Subaihi_2002] <br>not available for MMR",
                       "$\\textit{F test}$ for UMR and $\\textit{Approximate F test}$ for MMR",
                       "$1 - \\frac{SSE}{SST}$ <br>not available for MMR",
                       "$1 - \\frac{(n-1)(1-R^2)}{n-p}$ <br>[@Darlington_1968; @Judge_1985] <br>not available for MMR")

Formula_in_Logit_Cox_Poisson_Gamma <- c("logit, cox, poisson and gamma",
                                        "$-2\\text{LL} + 2p$ <br>[@Darlington_1968; @Judge_1985]",
                                        "$-2\\text{LL} + \\frac{n(n+p)}{n-p-2}$ <br>[@Hurvich_Tsai_1989]",
                                        "not available",
                                        "not available",
                                        "$-2\\text{LL} + 2p\\ln(\\ln(n))$ <br>[@Hannan_Quinn_1979]",
                                        "$-2\\text{LL} + p$ <br>[@Nelder_Wedderburn_1972; @Smith_Spiegelhalter_1980]",
                                        "$-2\\text{LL} + \\frac{3}{2}p$ <br>[@Smith_Spiegelhalter_1980]",
                                        "$-2\\text{LL} + p\\ln(n)$ <br>[@Schwarz_1978; @Judge_1985]",
                                        "Forward: LRT and Rao Chi-square test (logit, poisson, gamma); LRT (cox); <br><br>Backward: Wald test",
                                        "not available",
                                        "not available")
df <- data.frame(Abbreviation, Definition, Formula_in_Linear, Formula_in_Logit_Cox_Poisson_Gamma)
colnames(df) <- c("Abbreviation","Definition","Formula","")

kable(df, format = "html", align = "l", 
      booktabs = TRUE, escape = F, 
      caption = 'Abbreviation, Definition, and Formula of the Selection Metric for Linear, Logit, Cox, Possion, and Gamma regression') %>%
  footnote(number = c("Unsupported AIC formula (which does not affect the selection process as it only differs by constant additive and multiplicative factors):\n
                      $AIC=n\\ln\\left(\\frac{SSE}{n}\\right) + 2p$ [@Darlington_1968; @Judge_1985]", 
                      "Unsupported AICc formula (which does not affect the selection process as it only differs by constant additive and multiplicative factors):\n
                      $AICc=\\ln\\left(\\frac{SSE}{n}\\right) + 1 + \\frac{2(p+1)}{n-p-2}$ [@McQuarrie_Tsai_1998]")) %>%
  kable_styling() %>%
  column_spec(3, width = "0.5in") %>%
  column_spec(4, width = "0.4in")
```

No metric is necessarily optimal for all datasets. The choice of them depends on your data and research goals. We recommend using multiple metrics simultaneously, which allows the selection of the best model based on your specific needs. Below summarizes general guidance.

+ AIC: AIC works by penalizing the inclusion of additional variables in a model. The lower the AIC, the better performance of the model. AIC does not include sample size in penalty calculation, and it is optimal in minimizing the mean square error of predictions [@Brewer_2016].

+ AICc: AICc is a variant of AIC, which works better for small sample size, especially when `numObs / numParam < 40` [@Burnham_2002].

+ Cp: Cp is used for linear models. It is equivalent to AIC when dealing with Gaussian linear model selection.

+ IC(1) and IC(3/2): IC(1) and IC(3/2) have 1 and 3/2 as penalty factors respectively, compared to 2 used by AIC. As such, IC(1) turns to return a complex model with more variables that may suffer from overfitting issues.

+ BIC and SBC: Both BIC and SBC are variants of Bayesian Information Criterion. The main distinction between BIC/SBC and AIC lies in the magnitude of the penalty imposed: BIC/SBC are more parsimonious when penalizing model complexity, which typically results to a simpler model [@SAS_Institute_2018; @Sawa_1978; @Hurvich_Tsai_1989; @Schwarz_1978; @Judge_1985; @Al-Subaihi_2002].

The precise definitions of these criteria can vary across literature and in the SAS environment. Here, BIC aligns with the definition of the Sawa Bayesion Information Criterion as outlined in  [SAS](https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.3/statug/statug_glmselect_syntax07.htm) documentation, while SBC corresponds to the Schwarz Bayesian Information Criterion. According to Richard's [post](https://www.linkedin.com/pulse/aicbic-model-selection-richard-randa/), whereas AIC often favors selecting overly complex models, BIC/SBC prioritize a small models. Consequently, when dealing with a limited sample size, AIC may seem preferable, whereas BIC/SBC tend to perform better with larger sample sizes.

+ HQ: HQ is an alternative to AIC, differing primarily in the method of penalty calculation. However, HQ has remained relatively underutilized in practice [@Burnham_2002].

+ Rsq: The R-squared (R²) statistic measures the proportion of variations that is explained by the model. It ranges from 0 to 1, with 1 indicating that all of the variability in the response variables is accounted for by the independent variables. As such, R-squared is valuable for communicating the explanatory power of a model. However, R-squared alone is not sufficient for selection because it does not take into account the complexity of the model. Therefore, while R-squared is useful for understanding how well the model fits the data, it should not be the sole criterion for model selection.

+ adjRsq: The adjusted R-squared (adj-R²) seeks to overcome the limitation of R-squared in model selection by considering the number of predictors. It serves a similar purpose to information criteria, as both methods compare models by weighing their goodness of fit against the number of parameters. However, information criteria are typically regarded as superior in this context [@Stevens_2016].

+ SL: SL stands for Significance Level (P-value), embodying a distinct approach to model selection in contrast to information criteria. The SL method operates by calculating a P-value through specific hypothesis testing. Should this P-value fall below a predefined threshold, such as 0.05, one should favor the alternative hypothesis, indicating that the full model significantly outperforms the reduced model. The effectiveness of this method hinges upon the selection of the P-value threshold, wherein smaller thresholds tend to yield simpler models.

## Multicollinearity

This [blog](https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/) by Jim Frost gives an excellent overview of multicollinearity and when it is necessary to remove it.

Simply put, a dataset contains multicollinearity when input predictors are correlated. When multicollinearity occurs, the interpretability of predictors will be badly affected because changes in one input variable lead to changes in other input variables. Therefore, it is hard to individually estimate the relationship between each input variable and the dependent variable.

Multicollinearity can dramatically reduce the precision of the estimated regression coefficients of correlated input variables, making it hard to find the correct model. However, as Jim pointed out, “Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don’t need to understand the role of each independent variable, you don’t need to reduce severe multicollinearity.”

In StepReg, [QC Matrix Decomposition](https://towardsdatascience.com/qr-matrix-factorization-15bae43a6b2#:~:text=The%20QR%20matrix%20decomposition%20allows%20one%20to%20express%20a%20matrix,zero%2C%20it%20is%20also%20invertible.) is performed ahead of time to detect and remove input variables causing multicollinearity.

# StepReg Output

A list containing multiple tables will be returned. 

```{r, echo = FALSE}
Table <- c(
"Summary of Parameters",
"Variables and Types",
"Selection Process under a given strategy with metrics",
"Parameter Estimates for y under a given strategy with metrics")

Meanings <- c(
"The parameters used in stepwise regression along with their default or user-specified values",
"The variables and their respective types in the dataset",
"The detailed overview of the variable selection process under a given strategy with metrics",
"The parameter estimates for the optimal models under a given strategy with metrics")

kable_styling(kable(data.frame(Table,Meanings),format = "html", align='l', escape = F, caption = 'Output Explanation of StepReg')) #%>% column_spec(1:2, width = c("2in","4in")))
```

# Demo

In this section, we provided some examples using distinct parameters across various regression scenarios with the 4 datasets, a.k.a. mtcars, remission, lung, and CreditCard.

## linear stepwise regression with _mtcars_

-   [**mtcars**](https://cran.r-project.org/web/packages/explore/vignettes/explore_mtcars.html): the mtcars dataset is a classic automotive dataset that provides information on various car models and their performance attributes. With 32 observations and 11 variables, it includes details such as miles per gallon (mpg), horsepower(hp), and the number of cylinders(cyl). For more information, please ```?mtcars```

> **Example1**:  
>
> + type of regression: `linear`
>
> + response: `mpg`
>
> + predictors: all variables except `mpg` 
>
> + variable selection strategy: `forward` and `backward`
>
> + selection metric: `AIC` 
>
> + force `disp` and `cyl` to be included in all models.

```{r}
    library(StepReg)
    data(mtcars)
    formula <- mpg ~ .
    res1 <- stepwise(formula = formula,
                      data = mtcars,
                      type = "linear",
                      include = c("disp","cyl"),
                      strategy = c("forward","backward"),
                      metric = "AIC")
    res1
```

> Visulization of the selection process under `AIC`.

```{r plot_res1, warning=FALSE}
    plot(res1)
```

> **Example2**: 
>
> + type of regression: `linear`
>
> + response: `mpg` 
>
> + predictors: all other variables except `mpg` and `intercept`. 
>
> + variable selection strategy: `bidirectional` 
>
> + selection metric: run `AIC`, `AICc`, `BIC`,`HQ`, `HQc`, `SBC`, and `SL` parallelly, and the significance levels for entry (`sle`) and stay (`sls`) were both set to 0.05 . 
>
> + Users can compare output within each metic through the output table and visualization.

```{r}
    formula <- mpg ~ . + 0
    res2 <- stepwise(formula = formula,
                      data = mtcars,
                      type = "linear",
                      strategy = "bidirection",
                      metric = c("AIC","SBC","SL","AICc","BIC","HQ"),
                      sle = 0.05,
                      sls = 0.05)
    res2
```

> + Visulization of the selection process using `bidirection` strategy under information criteron `AIC`, `AICc`, `BIC`,`HQ`, `SBC`, and `SL` with `sle`=0.05 and `sls`=0.05. 

```{r plot_res2, warning=FALSE}
    plot(res2)
```

> **Example3**: 
>
> + type of regression: `linear`
>
> + response: multivariates of `mpg` and `drat` 
>
> + predictors: `cyl`, `disp`, `hp`, `wt`, `vs`, `am` and `intercept`. 
>
> + variable selection strategy: `subset` 
>
> + selection metric: run `AIC`, `SBC` and `HQ` parallelly

```{r}
    formula <- cbind(mpg,drat) ~ cyl + disp + hp + wt + vs + am
    res3 <- stepwise(formula = formula,
                      data = mtcars,
                      type = "linear",
                      include = 'wt',
                      strategy = "subset",
                      metric = c("AIC","AICc","SBC","HQ"),
                      best_n = 3)
    res3
```

> + Visulization of the selection process using `subset` strategy under information criteron `AIC`, `SBC` and `HQ`.

```{r plot_res3, warning=FALSE}
    plot(res3)
```

## Logistic stepwise regression with `remission`

-   [**remission**](https://online.stat.psu.edu/stat501/book/export/html/1011): the remission dataset is relevant in the context of medical research, specifically in oncology. It captures data related to the remission status of leukemia patients. With 27 observations and 7 variables, the dataset includes variables such as remission status (1 for remission and 0 for non-remission), cellularity of the marrow clot section(cell), and the highest temperature before the start of treatment(temp). For more information, please ```?StepReg::remission```


> **Example4**: 
>
> + type of regression: `logit`
>
> + response: `remiss` 
>
> + predictors: all variables except `remiss`
>
> + variable selection strategy: `forward` 
>
> + selection metric: run `AIC` and `SL` parallelly, where `sle` and `sls` were both set to 0.05.
>
> + force `cell` always in the model.

```{r warning = FALSE}
    data(remission)
    formula <- remiss ~ .
    res4 <- stepwise(formula = formula,
                      data = remission,
                      type = "logit",
                      include= "cell",
                      strategy = "forward",
                      metric = c("AIC","SL"),
                      sle = 0.05)
    res4
```

> + Visulization of the selection process using `forward` strategy under information criteron `AIC` and `SL`.

```{r plot_res4, warning=FALSE}
    plot(res4)
```

> **Example5**: 
>
> + type of regression: `logit`
>
> + response: `remiss` 
>
> + predictors: all variables except `remiss`
>
> + variable selection strategy: `subset` 
>
> + selection metric: run `SBC` and `SL` parallelly, where `sle` and `sls` used default 0.15.

```{r warning = FALSE}
    data(remission)
    formula <- remiss ~ .
    res5 <- stepwise(formula = formula,
                      data = remission,
                      type = "logit",
                      strategy = "subset",
                      metric = c("SBC","SL"),
                      best_n = 3)
    res5
```

> + Visulization of the selection process using `subset` strategy under information criteron `SBC` and `SL`.

```{r plot_res5, warning=FALSE}
    plot(res5)
```

## Cox stepwise regression with `lung`

-   [**lung**](https://stat.ethz.ch/R-manual/R-devel/library/survival/html/lung.html): the lung dataset is a dataset in the survival analysis domain, containing information related to the survival times of 228 patients with advanced lung cancer. It includes variables such as the patient's age, the type of treatment received, and survival status. For more information, please ```?survival::lung```

> **Example6**: 
>
> + type of regression: `cox`
>
> + response: `Surv(time, status_binary)`
>
> + predictors: all variables except `status`
>
> + variable selection strategy: `forward` 
>
> + selection metric: run `IC(1)` and `SL` parallelly, where `sle` was set to 0.05.
>
> + force `age` in all models.

```{r warning = FALSE}
    lung <- survival::lung
    lung_noNA <- na.omit(lung)
    lung_noNA$status_binary <- ifelse(lung_noNA$status == 2,1,0)
    formula  =  Surv(time, status_binary) ~ . - status
    
    res6 <- stepwise(formula = formula,
                      data = lung_noNA,
                      type = "cox",
                      include = "age",
                      strategy = "forward",
                      metric = c("IC(1)","SL"),
                      sle = 0.05)
    res6
```

> + Visulization of the selection process using `forward` strategy under information criteron `IC(1)` and `SL`.

```{r plot_res6, warning=FALSE}
    plot(res6)
```

> **Example7**: 
>
> + type of regression: `cox`
>
> + response: `Surv(time, status_binary)`
>
> + predictors: all variables except `status`
>
> + variable selection strategy: `backward` 
>
> + selection metric: run `SL` and `AIC` parallelly, where `sls` was set to 0.05.

```{r warning = FALSE}
    formula = Surv(time, status_binary) ~ . - status 
    res7 <- stepwise(formula = formula,
                      data = lung_noNA,
                      type = "cox",
                      strategy = "backward",
                      metric = c("SL","AIC"),
                      sls = 0.05)
    res7
```

> + Visulization of the selection process using `backward` strategy under information criteron `AIC` and `SL`.

```{r plot_res7, warning=FALSE}
    plot(res7)
```

## Poisson stepwise regression with `CreditCard`

-   [**CreditCard**](https://search.r-project.org/CRAN/refmans/AER/html/CreditCard.html): the CreditCard dataset is associated with credit risk analysis and financial research. It contains information about credit card transactions, including details such as the amount spent, credit limit, and payment status. For more information, please ```?AER::CreditCard```

> **Example8**: 
>
> + type of regression: `poisson`
>
> + response: `reports`
>
> + predictors: all variables except `reports`
>
> + variable selection strategy: `forward` 
>
> + selection metric: run `SL` and `IC(3/2)` parallelly, and `sle` was set to 0.05.

```{r warning = FALSE}
    data(CreditCard, package = 'AER')
    formula  = reports ~ .
    
    res8 <- stepwise(formula = formula,
                      data = CreditCard,
                      type = "poisson",
                      strategy = "forward",
                      metric = c("SL","IC(3/2)"),
                      sle=0.05)
    res8
```

> + Visulization of the selection process using `forward` strategy under information criteron `IC(3/2)` and `SL`.

```{r plot_res8, warning=FALSE}
    plot(res8)
```

> **Example9**: 
>
> + type of regression: `poisson`
>
> + response: `reports`
>
> + predictors: all variables except `reports`
>
> + variable selection strategy: `bidirection` 
>
> + selection metric: run `AIC` and `IC(3/2)` parallelly
>
> + force `card` and `months` in all models.

```{r warning = FALSE}
    formula = reports ~ .
    res9 <- stepwise(formula = formula,
                      data = CreditCard,
                      type = "poisson",
                      include=c("card","months"),
                      strategy = "bidirection",
                      metric = c("IC(3/2)","AIC")
                      )
    res9
```

> + Visulization of the selection process using `bidirection` strategy under information criteron `IC(3/2)` and `AIC`.

```{r plot_res9, warning=FALSE}
    plot(res9)
```

# Session Info

```{r sessionInfo, echo = FALSE}
sessionInfo()
```